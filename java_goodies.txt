**********Java***********

ClassLoaders:

There are three types of built-in Class Loaders in Java:
1. Bootstrap Class Loader – It loads JDK internal classes, typically loads rt.jar and other core classes for example java.lang.* package classes
2. Extensions Class Loader – It loads classes from the JDK extensions directory, usually $JAVA_HOME/lib/ext directory.
3. System Class Loader – It loads classes from the current classpath that can be set while invoking a program using -cp or -classpath command line options.

We use a custom classloader when we need to extend how the JVM loads the classes. By default it loads from the -classpath (from the folder of the project).
But in some cases we may want to load a class from a database or from a remote resource (through http!).

The flow of "loadClass()" is as follows:

1. Verify class name.
2. Check to see if the class requested has already been loaded.
3. Check to see if the class is a "system" class.
4. Attempt to fetch the class from the parent class loader
5. Attempt to fetch the class from this class loader's repository.
6. Define the class for the VM.
7. Resolve the class.
8. Return the class to the caller.

Always load the classes once. It's also important to keep a cache because the loadClass() method is called recursively when a class is
being resolved, and you will need to return the cached result rather than chase it down for another copy.

Classpath: contains folders (like src/main/java) and jar files. A java class is found like this:
 - if a jar contains it, it is loaded from there
 - if not, the JVM concatenates the folder path from the classpath with the "import ..." statement located in the class file. 
	The default classpath is the current working directory.
   The resulting path must point to the .class file.
   
   
   
 **********Eclipse**********
 
 The build path:
	contains folders, that are part of the classpath along with .jar libraries - individual jars, maven/gradle jars.
	These folders are tipically: src/main/java, src/main/resources, src/test/java. The order matters, the classloader tries the src/main/java first.
		Order can be changed in right click --> Properties --> Java Build Path --> Order and Export
		
		
Facets: this tells eclipse what our project is capable of. Right click --> Properties --> Project facets.
	Only those projects can be added for example to Tomcat that has
		- Java facet
		- Dynamic Web Module facet: if the project has this the Properties will show the "Deployment Assembly" menu which tells eclipse where to put certain
			folders (like /src/main/java) in the output .war file. Usually there is a "WebContent" folder as well located in /src/main/WebContent, however Spring does not require this.
			Only old-school web archetype projects are scaffolded with this folder (it is for holding the static files: html, css, js. Also the web.xml, other necessary config xml files).
			If no web.xml was added, then add it manually: in Project Explorer: Deployment Descriptor right click --> Generate Deployment Descriptor Stub
			
			Also:
				- this facet makes eclipse display our application on the popup shown when trying to add our application to the ones that get started when Tomcat starts: Servers tab --> Tomcat --> Right click --> "Add and Remove..." - now out application is displayed
				- this faces makes eclipse able to run our application on server: On the project --> Right click --> Run as --> Run on Server. Now our application starts.
			
			This facet also adds the web.xml deployment descriptor. This is where filters/servlets FilterConfigs, ServletConfigs can be specified using 2.5 servlet-container.
			3.0+ servlet-containers support annotation-based Servlet, Filters and their configs, init-params without the need for a web.xml.
			In maven the fail on missing web.xml can be turned off:
				<build>
					<plugins>
						<plugin>
							<groupId>org.apache.maven.plugins</groupId>
							<artifactId>maven-war-plugin</artifactId>
							<version>2.6</version>
							<configuration>
								<failOnMissingWebXml>false</failOnMissingWebXml>
							</configuration>
						</plugin>
					</plugins>
				</build>
				
				
				
				
**********IntelliJ**********

If you want to start more instances of a spring boot app, you just need to do 2 things:
	1. in the "Edit configurations" uncheck the "Single instance only" checkbox
	2. in application.properties set server.port=0 (so to zero)
	
	With this, when you start the application IntelliJ assignes a random port. And the app can be started multiple times.
	
	
	
	
**********Spring**********
server.context-path=/champion-portal
	this is like virtual path in Asp.Net applications: our app will be available at http://localhost:8080/champion-portal - this gets to be the root




**********Maven**********

a plugin is a collection of goals with a general common purpose

mvn archetype:generate 
	- plugin:goal: execute the "generate" goal of the "archetype plugin"

mvn package - Rather than a goal, this is a phase
	A phase is a step in the build lifecycle, which is an ordered sequence of phases. A phase consists of plugin goals.
	When a phase is given, Maven will execute every phase in the sequence up to and including the one defined. Every goal in these cases is executed.
	For example, if we execute the compile phase, the phases that actually get executed are:
		validate
		generate-sources
		process-sources
		generate-resources
		process-resources
		compile

mvn -DskipTests=true clean install <-- to excute the "clean" phase of the "clean" lifecycle then the "build" lifecycle's "install" phase but skip the preceeding "test" phase

mvn spring-boot:run: to run a spring boot project (pom.xml is required ofc)
mvn spring-boot:run --debug

Lifecycles:
 - clean: cleans up artifacts created by prior builds
 - build: (and its default phases are:)	validate: validate the project is correct and all necessary information is available
										compile: compile the source code of the project
										test: test the compiled source code using a suitable unit testing framework. These tests should not require the code be packaged or deployed
										package: take the compiled code and package it in its distributable format, such as a JAR.
										integration-test: process and deploy the package if necessary into an environment where integration tests can be run
										verify: run any checks to verify the package is valid and meets quality criteria
										install: install the package into the local repository, for use as a dependency in other projects locally
										deploy: done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects.
- site: generates site documentation for this project

An interesting thing to note is that phases and goals may be executed in sequence:
	mvn clean dependency:copy-dependencies package - This command will clean the project, copy dependencies, and package the project (executing all phases up to package, of course).

A Build Lifecycle is Made Up of Phases
A Build Phase is Made Up of Plugin Goals


To install an external jar into a project-repository:
mvn install:install-file -DlocalRepositoryPath=repo -DcreateChecksum=true -Dpackaging=jar -Dfile="lib/overall-lib-0.0.1.jar" -DgroupId=overall-lib -DartifactId=overall-lib -Dversion="0.0.1" -e

When creating a maven multi-project app (parent's packaging is "pom") it is very important to always build the parent pom.
That will build all of the modules individually and resolve the dependencies (ensuring the correct order of the build of projects).
As soon as a modules starts depending on another, that module cannot be built from its pom. The parent pom will be the only one that can build it from that point on.


Maven plugins (build and reporting) are configured by specifying a <configuration> element where the child elements of the <configuration> element 
are mapped to fields, or setters, inside your Mojo (remember that a plug-in consists of one or more Mojos where a Mojo maps to a goal). 
Say, for example, we had a Mojo that performed a query against a particular URL, with a specified timeout and list of options.
https://maven.apache.org/guides/mini/guide-configuring-plugins.html


How do you know Jackson 2 is on the classpath? 
Either run `mvn dependency:tree` or './gradlew dependencies' and you'll get a detailed tree of dependencies which shows Jackson 2.x. 
You can also see that it comes from spring-boot-starter-web.

To update dependencies shown in eclipse: right click on the project --> Maven --> Update Project... (Maven tends to do this on his own but not always :-()


Dependency Scope:
	- compile: This is the default scope, used if none is specified. Compile dependencies are available in all classpaths of a project. Furthermore, those dependencies are propagated to dependent projects. Gets copied to the final output.
	- provided: This is much like compile, but indicates you expect the JDK or a container to provide the dependency at runtime. For example, when building a web application for the Java Enterprise Edition, you would set the dependency on the Servlet API and related Java EE APIs to scope provided because the web container provides those classes. This scope is only available on the compilation and test classpath, and is not transitive. Does not get copied to the final output.
	- runtime: This scope indicates that the dependency is not required for compilation, but is for execution. It is in the runtime and test classpaths, but not the compile classpath. Gets copied to the output.
	- test: This scope indicates that the dependency is not required for normal use of the application, and is only available for the test compilation and execution phases. This scope is not transitive.
	- system: This scope is similar to provided except that you have to provide the JAR which contains it explicitly. The artifact is always available and is not looked up in a repository.
	- import: This scope is only supported on a dependency of type pom in the <dependencyManagement> section. It indicates the dependency to be replaced with the effective list of dependencies in the specified POM's <dependencyManagement> section. Since they are replaced, dependencies with a scope of import do not actually participate in limiting the transitivity of a dependency.
	

**Maven vs gradle**: 
	- maven can use dependencies only from the maven repository. 3rd party jars cannot be used, unless installing them to a local maven repository prior.
	- gradle can use deps from the the maven repository and from the file sytem (3rd party legacy jars)
		- compile("mysql:mysql-connector-java:5.1.38") 	- group:artifactId:version
		- compile files("libs/sqljdbc4-4.2.jar")		- this is relative path, but absolute can be given as well
		- compile project(':VMCPriceDetector')			- multi gradle project
		
		

**********Gradle**********

build.gradle file
The eclipse plugins adds 2 nice views: Gradle Executions and Gradle Tasks to be able to run the desired gradle task without:
	1. going to the command line
	2. the need to use the Run Configurations... window where we can add a new "Gradle Project"

	gradle --help: to display the help window
	gradle tasks: to display the available tasks (plugins might add tasks, so does the "java" plugin)
	gradle <task>: to run a task

	gradle clean: deletes the build directory
	gradle jar: assemble a jar achrive containing the main classes
	gradle javadoc: generates javadoc
	gradle build: assembles and tests the project
	gradle check: run all checks
	gradle test: run the unit tests

	gradle bootRun: to run a spring-boot project (build.gradle required ofc)

	gradle clean jar: to run the "clean" and after that the "jar" tasks - java plugins adds them

To execute the wrapper specific to the project instead of the globbaly installed gradle version use:
	gradlew.bat <task>
	
To execute in a multi-project environment a task only on one specific project use this:
	gradlew.bat :edge-service:build - still from the root folder, where "edge-service" is the name of the target project
	
To specify the gradle of the wrapper to be used:
	task wrapper(type: Wrapper) {
		gradleVersion = '3.2.1'
		distributionType = Wrapper.DistributionType.ALL
	}
	
To download the wrapper:
	gradle wrapper --gradle-version 3.2.1
	
To ignore tasks during build (in this case "generateTypeScript") won't get executed:
	gradlew assemble -x generateTypeScript


https://docs.gradle.org/current/userguide/tutorial_gradle_command_line.html
Example multiple tasks:
	task compile << {
		println 'compiling source'
	}

	task compileTest(dependsOn: compile) << {
		println 'compiling unit tests'
	}

	task test(dependsOn: [compile, compileTest]) << {
		println 'running unit tests'
	}

	task dist(dependsOn: [compile, test]) << {
		println 'building the distribution'
	}

	
A little explanation about the build.gradle file's structure:

- there are plugins that expose configurations and we can specify configuration sections to adjust them:
build.gradle: (of VMCPriceCheckerAdmin)

	eclipse {
		classpath {
			 containers.remove('org.eclipse.jdt.launching.JRE_CONTAINER')
			 containers 'org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/JavaSE-1.8'
		}
	}
	sonarqube {
		properties {
			property "sonar.projectVersion", "1.0"
			property "sonar.jacoco.reportPath", "${buildDir}/jacoco/test.exec"
			property "sonar.java.source", "1.8"
			property "sonar.java.target", "1.8"
		}
	} 
	
	The "eclipse" and "sonarqube" are config sections exposed by the corresponding plugins:
		plugins{
			id "org.sonarqube" version "1.1"
		}

		apply plugin: 'java'
		apply plugin: 'eclipse'
		apply plugin: 'spring-boot' 
		apply plugin: 'jacoco'
	
	Dependencies:
		dependencies {
			// Mysql and Mssql connection
			compile('mysql:mysql-connector-java:5.1.38')
			compile files('libs/sqljdbc4-4.2.jar')
			
			compile project(':VMCPriceDetector')
			
			// Mybatis
			compile("org.mybatis.spring.boot:mybatis-spring-boot-starter:1.1.1")
			
			// RxJava 
			compile("io.reactivex:rxjava:1.1.7")	// maven's compile
			
			providedRuntime("org.springframework.boot:spring-boot-starter-tomcat")	// maven's provided
			runtime("...")	// maven's runtime
			
			testCompile("org.springframework.boot:spring-boot-starter-test") 
			testCompile("org.hsqldb:hsqldb:2.3.4")
		}
		
		// hooking into the "test" task: this method will be called before the "test" task itself - the java plugin provides the "test" task
		// note: chaining it to for instance "test2" would make the build instantly fail since there is no "test2" function defined before 
		test {
			println "- I'm the largest animal that has ever lived on this planet."
		}
		
	task wrapper(type: Wrapper) {
		gradleVersion = '3.2.1'
	}
		
settings.gradle:
	include ':VMCPriceDetector'
	project(':VMCPriceDetector').projectDir = new File(settingsDir, '../VMCPriceDetector')

	VMCPriceCheckerAdmin
	|
	|_ _build.gradle
	|
	|_ _VMCPriceDetector
		|
		|...
	
	To build VMCPriceCheckerAdmin and all of its dependant projects (currently only VMCPriceDetector):
		gradlew build
	To build only the VMCPriceDetector dependency (from the same directory):
		gradlew :VMCPriceDetector:build

		
		
	A little more on dependencies: after modifying the dependencies, in Eclipse a refresh is necessary to make Eclipse update the jars displayed 
		in "Project and External Dependencies" library:
		
		right click on the project --> Gradle --> Refresh Gradle Project (this is similar to Maven's Update Project...)
		
How to build multiple projects in gradle (include dependencies and their transitive dependencies into the build output): 
	https://docs.gradle.org/current/userguide/multi_project_builds.html
	
	
	
******Dependency management*******

A Maven bom is a so called "bill of materials" - it bundles several dependencies to assure that the versions will work together.
	You include the bom into your pom like this:
	<dependencyManagement>
		<dependencies>
			<dependency>
				<groupId>org.jboss.bom</groupId>
				<artifactId>jboss-javaee-6.0-with-tools</artifactId>
				<version>${javaee6.with.tools.version}</version>
				<type>pom</type>
				<scope>import</scope>
			</dependency>
		</dependencies>
	</dependencyManagement> 
	
	Then you do not have to specify the version attribute of a dependency, if it is defined in the bom like this:
	<dependency>
		<groupId>javax.enterprise</groupId>
		<artifactId>cdi-api</artifactId>
		<scope>provided</scope>
	</dependency>
	
Gradle dependency management plugin: https://github.com/spring-gradle-plugins/dependency-management-plugin
	
	Dependency management DSL: this configuration will cause all dependencies (direct or transitive) on spring-core and commons-logging to have the versions 4.0.3.RELEASE and 1.1.2 respectively.
		dependencyManagement {
			 dependencies {
				  dependency 'org.springframework:spring-core:4.0.3.RELEASE'
				  dependency group:'commons-logging', name:'commons-logging', version:'1.1.2'
			 }
		}
		dependencies {
			 compile 'org.springframework:spring-core'
		}
		
	Exclusions:
		dependencyManagement {
			dependencies {
				dependency('org.springframework:spring-core:4.0.3.RELEASE') {
					exclude 'commons-logging:commons-logging'
				}
			}
		}
	
	Importing a maven bom --> dependency versions come from this bom file --> we can just use: "compile 'org.springframework:spring-core'"
	
		dependencyManagement {
			 imports {
				  mavenBom 'io.spring.platform:platform-bom:1.0.1.RELEASE'
			 }
		}
		dependencies {
			 compile 'org.springframework.integration:spring-integration-core'
		}
		// we can override the versions used and define variables
		buildscript {
		ext {
			springBootVersion = "1.5.1.RELEASE"	// define variable
			gradlewVersion = "3.3"
			jooqVersion = "3.9.1"
		}
		ext["hibernate.version"] = "5.2.6.Final"	// override version used by the dependency plugin
		
	Gradle's maven and maven-publish plugins automatically generate a pom file that describes the published artifact.
	
	
**********Java 8 Date API**********

ZonedDateTime, LocalDateTime, LocalDate, LocalTime, ZoneOffset, Duration, Clock, Instant - java.time package

	Use ZonedDateTime if offset information is needed because only this one contains it (like DateTimeOffset is C#)
	After sent to the client (in json) the browser can easily convert it to javascript Date object.
	You do not need to insist on UTC datetimes, local datetime is good as well, it only has to be a ZonedDateTime.

		ZonedDateTime now = ZonedDateTime.now();
		model.addAttribute("now", now.toInstant());
		// .toInstant() call is necessary in order to get the desired output sent to the browser that can be mapped into a javascript Date.
		// now().toString(): 2017-01-24T16:59:28.484+01:00[Europe/Prague]
		// now().toInstant().toString(): 2017-01-24T15:59:28.484Z - and this is the good one required by the browsers

	Please avoid the old java.util.Date and Calendar classes!
	
		

**********Tomcat**********

tomcat-users.xml:
	<role rolename="admin-gui"/>
	<role rolename="manager-gui"/>
	<user username="admin" password="admin" roles="admin-gui,manager-gui"/>

	Specifies the users and their roles. Tomcat comes with default roles. For example the "manager-gui" role is required to be able to access the "Manager app" (http://localhost:8080/manager/html)
	
	
server.xml

	the port Tomcat gets started on can be changed here.
	
Deploy:
	war files can be deployed in two ways:
	
		1. copy the .war into the "webapps" folder. Context path will be the war's name.
		2. Launch tomcat and use the "Manager app" to deploy a war. The user can optionally change the context path here.
			In order to be able to use the "Manager app" a user with "manager-gui" role must be added to the tomcat-users.xml file, like this: <user username="admin" password="admin" roles="tomcat,manager-gui"/>
			
To run Tomcat:

	/bin/startup.bat	<-- runs tomcat

	Note: running Tomcat from Eclipse does not allow us to use the default apps that come with Tomcat (like the manager app)
	Double-clicking on the Tomcat server in the "Servers" view in Eclispe allows us to change some settings (like the port on the "Overview" tab and the context path of the deployed applications on the "Modules" tab), 
		but THIS HAS NO EFFECT on the real server.xml and other files which are taken into consideration when starting Tomcat from the /bin folder instead of using Eclipse. So it has no effect on production environment.
		
		
**********JVM parameters***********

Pre-allocated memory management:
	-Xms64m
	-Xmx512m
	-XX:MetaspaceSize=96m
	-XX:MaxMetaspaceSize=256



**********Servlet**********

Request, Response objects - created per access
Servlet itself: not per access, shared accross different threads (1 thread / user)
HttpSession: 1 session object / user, machine - user specific
ServletContext - shared accross users (application-level) - 1 instance per application

GenericService	- init(ServletConfig) - for the very 1st request only
	|
HTTPServlet		- servlet() - at every request
	|
MyServlet		- doGet, doPost (at every request)


Filter interface:

void init(FilterConfig paramFilterConfig) – When container initializes the Filter, this is the method that gets invoked. This method is called only once in the lifecycle of filter and we should initialize any resources in this method. FilterConfig is used by container to provide init parameters and servlet context object to the Filter. We can throw ServletException in this method.
doFilter(ServletRequest paramServletRequest, ServletResponse paramServletResponse, FilterChain paramFilterChain) – This is the method invoked every time by container when it has to apply filter to a resource. Container provides request and response object references to filter as argument. FilterChain is used to invoke the next filter in the chain. This is a great example of Chain of Responsibility Pattern.
void destroy() – When container offloads the Filter instance, it invokes the destroy() method. This is the method where we can close any resources opened by filter. This method is called only once in the lifetime of filter.

I can listen to events:
- ServletContextListener
- ServletContextAttributeListener
- HttpSessionListener
- HttpSessionAttributeListener




**********JSP***********

7.1 What Are Custom Tags?
	Custom tags, also known as tag extensions, are JSP elements that allow custom logic and output provided by other Java components to be inserted into JSP pages. 
	The logic provided through a custom tag is implemented by a Java object known as a tag handler. When OC4J encounters a custom tag in a JSP during translation, it generates code to obtain and 
	interact with the tag handler. Custom tags are included in a JSP page using XML syntax. Tags may or may not contain a body. Tags can also contain XML attributes that match properties in the corresponding tag handler.

With the advent of JSP 2.0, you now have two options for creating custom tags:

Tag handlers
	Tags that require the creation of tag handler classes come in two types: Classic and simple.
	Classic tag handlers have been available since JSP 1.1. Classic tags are considered somewhat cumbersome to write, in part because of the complexity of the Java interfaces used to implement each tag's corresponding tag handler class. They are also dependent on Java expressions for dynamic attribute values. However, these tag handlers are the only option if Java scriplets or expressions must be used in the tag body
	Simple tag handlers are new in JSP 2.0, and offer a much simpler lifecycle and interface than classic tag handlers. Tag bodies accept JSP expression language (EL) expressions, allowing completely script-free tag development.

Tag files
	Also new in JSP 2.0, tag files are revolutionary in that they allow tag libraries to be implemented completely in JSP or XML syntax, without the need to create and compile tag handler classes. Instead, tag files are translated into simple tag handlers by the OC4J JSP container and then compiled. Because of their ease of implementation, tag files offer an attractive alternative to writing tag handlers.

A tag file is a source file that contains a fragment of JSP code that is reusable as a custom tag. Tag files allow you to create custom tags using JSP syntax. 
Just as a JSP page gets translated into a servlet class and then compiled, a tag file gets translated into a tag handler and then compiled.


<%@ include file="header.html" %>
Static: adds the content from the value of the file attribute to the current page at translation time. 
The directive was originally intended for static layout templates, like HTML headers.

<jsp:include page="header.jsp" />
Dynamic: adds the content from the value of the page attribute to the current page at request time. 
Was intended more for dynamic content coming from JSPs.

In the latter case we can specify parameters as well:
<jsp:include page="navMenu.jsp" >
    <jsp:param name="param1" value="menu" />
</jsp:include>
${param.param1} (preferable) or request.getParameter("param1")




********Streams*********

The parallel streams use the default ForkJoinPool which by default has one less threads as you have processors, as returned by Runtime.getRuntime().availableProcessors() (so parallel streams use all your processors because they also use the main thread):
For applications that require separate or custom pools, a ForkJoinPool may be constructed with a given target parallelism level; by default, equal to the number of available processors.

Streams should be used with high caution when processing intensive computation tasks. In particular, by default, all streams will use the same ForkJoinPool, configured to use as many threads as there are cores in the computer on which the program is running.
If evaluation of one parallel stream results in a very long running task, this may be split into as many long running sub-tasks that will be distributed to each thread in the pool. From there, no other parallel stream can be processed because all threads will be occupied. So, for computation intensive stream evaluation, one should always use a specific ForkJoinPool in order not to block other streams.

List<SomeClass> list = // A list of objects
Stream<SomeClass> stream = list.parallelStream().map(this::veryLongProcessing);
Callable<List<Integer>> task = () -> stream.collect(toList());
ForkJoinPool forkJoinPool = new ForkJoinPool(4);
List<SomeClass> newList = forkJoinPool.submit(task).get();




********Java generics*********
At runtime java forgets its generic information for instance variables. But it retains that information for subclasses!

List<String> x = new ArrayList<>(); - type information gets lost!
	List<String> x = new ArrayList<>();
	Class<? extends List> clazz = x.getClass();
	ParameterizedType type = (ParameterizedType) clazz.getGenericSuperclass();
	String name = type.getTypeName();
	System.out.println(name);	// prints out java.util.AbstractList<E>...

List<String> x = new ArrayList<String>() {}; - type information is retained due to we create a generic anonymous subclass of List<String> with no overrides!
	List<String> x = new ArrayList<String>() {};
	Class<? extends List> clazz = x.getClass();
	ParameterizedType type = (ParameterizedType) clazz.getGenericSuperclass();
	String name = type.getTypeName();
	System.out.println(name);	// prints out String!
	
	This is called the "Type token pattern" - the Spring implementation is ParameterizedTypeReference<T>
	

And by the way!
Generic information is kept for declared fields (so not for the instance but for the declaration), constructor parameters, method parameters and return types.
Type information is also preserved for extends and implements clauses.
So the generic information can be acquired through reflection:

	public class MyClass {
	
		private List<Integer> intList;
		
		private void testThisIsTrue() throws NoSuchFieldException, SecurityException {
			
			System.out.println(this.intList.getClass().getGenericSuperclass().getTypeName()); 					// prints java.util.AbstractList<E>
			System.out.println(this.getClass().getDeclaredField("intList").getGenericType().getTypeName());		// prints java.util.List<java.lang.Integer>
		}
	
	}
	
	
Type parameters "<T>" vs wildcard "?":

	Taking your method as example, suppose you want to ensure that the src and dest list passed to copy() method should be of same parameterized type, you can do it with type parameters like so:

	public static <T extends Number> void copy(List<T> dest, List<T> src)
	Here, you are ensured that both dest and src have same parameterized type for List. So, it's safe to copy elements from src to dest.

	But, if you go on to change the method to use wildcard:

	public static void copy(List<? extends Number> dest, List<? extends Number> src)
	it won't work as expected. In 2nd case, you can pass List<Integer> and List<Float> as dest and src. So, moving elements from src to dest wouldn't be type safe anymore. If you don't need such kind of relation, then you are free not to use type parameters at all.
	
	- If you have only one parameterized type argument, then you can use wildcard, although type parameter will also work.
	- Type parameters support multiple bounds, wildcards don't.
	
	Wildcards support both upper and lower bounds, type parameters just support upper bounds. So, if you want to define a method that takes a List of type Integer or it's super class, you can do:
		public void print(List<? super Integer> list) { } // OK
	but you can't use type parameter:
		public <T super Integer> void print(List<T> list) { }  // Won't compile
		
	However, upper bounds work:
		public void print(List<? extends Integer> list) { } // OK
		public <T extends Integer> void print2(List<T> list) { } // OK
		
	// PECS: "Producer Extends, Consumer Super"
		- "Producer Extends" - If you need a List to produce T values (you want to read Ts from the list), you need to declare it with ? extends T, e.g. List<? extends Integer>. But you cannot add to this list.
		
		- "Consumer Super" - If you need a List to consume T values (you want to write Ts into the list), you need to declare it with ? super T, e.g. List<? super Integer>. But there are no guarantees what type of object you may read from this list.
		
		- If you need to both read from and write to a list, you need to declare it exactly with no wildcards, e.g. List<Integer>.
		
		
	How this relates to C#:
	
		extends --> out: property reading, or function return type
			/// <typeparam name="T">
			/// The type of objects to enumerate. This type parameter is covariant. 
			/// That is, you can use either the type you specified or any type that is more derived. For more information about covariance and contravariance, see Covariance and Contravariance in Generics.
			/// </typeparam>
			public interface IEnumerable<out T> : IEnumerable
			{
			}
			
			/// <typeparam name="T1">
			/// The type of the first parameter of the method that this delegate encapsulates. This type parameter is contravariant. 
			/// That is, you can use either the type you specified or any type that is less derived. For more information about covariance and contravariance, see Covariance and Contravariance in Generics.
			/// </typeparam>
			/// <typeparam name="TResult">
			/// The type of the return value of the method that this delegate encapsulates. This type parameter is covariant. 
			/// That is, you can use either the type you specified or any type that is more derived. For more information about covariance and contravariance, see Covariance and Contravariance in Generics.
			/// </typeparam>
			public delegate TResult Func<in T1, out TResult>(T1 arg1);
			
		super	--> in: property writing, or function parameter
		
			/// <typeparam name="T1">
			///	The type of the first parameter of the method that this delegate encapsulates. This type parameter is contravariant. 
			/// That is, you can use either the type you specified or any type that is less derived. For more information about covariance and contravariance, see Covariance and Contravariance in Generics.
			/// </typeparam>
			public delegate void Action<in T1>(T1 arg1);

	class Worker {

		
		void consume(Consumer<? extends String> cons) {
			cons.accept("Hello");	// won't compile, ? needs to be "super"
		}
		<T extends String> void consume2(Consumer<T> cons) {
			cons.accept("Hello");	// the same, won't compile. However super cannot be used with type parameters.
		}
		void consume3(Consumer<? super String> cons) {
			cons.accept("Hello");    // Good.
		}
		
		void produce(Supplier<? extends String> prod) {
			final String s = prod.get();
		}
		<T extends String> T produce2(Supplier<T> prod) {
			final String s = prod.get();
			return prod.get();
		}

	}

	class WorkDelegator {
		
		WorkDelegator() {
			Worker worker = new Worker();
			
			// this::processor works for wildcard
			worker.process(this::processor);
			
			// and it also works for the C#-like generic
			worker.process2(this::processor);
		}

		// cannot use "?" here
		private <T extends String> void processor(T item) {
			...
		}
	}


	Get method parameter names by reflection at runtime:
		- works only on concrete classes from java 8
		- interfaces: there is no way to get the method parameter names for interface as of java 8 --> that is why we use annotations in Spring Data Rest Repositories, Spring Data JPA, Spring Data, MyBatis Mapper interfaces, etc.
		
		class Apple {
			public void calculateSomething(int banana, int orange) {
				... method parameters can be retained at runtime because apple is a class!
			}
		}
		
		interface Apple {
			void calculateSomething(@Param("banana") int banana, @Param("orange") int orange);	// <-- this is the trick! @Param but it can be anything else. @Param is used and processed, read by Spring Data.
		}
		
		
	********Static imports*********

	We can import static members from classes which allows us to use the member without the class qualification:

		import static hu.regens.edgemicro.feature.validation.Building.trySome;
		
		trySome();	// - in action
		
The static import declaration is analogous to the normal import declaration. Where the normal import declaration imports classes from packages, 
allowing them to be used without package qualification, the static import declaration imports static members from classes, 
allowing them to be used without class qualification.
	

Java vs C#:

	1. Classes and inner classes:

		Java:
			// notice how "Calculator.this" is used to reach the outer object from the inner:
			public class Calculator {

				private int howMany = 10;

				public class CalcInner {

					public void doSomething() {
						System.out.println(Calculator.this.howMany);	// or just type "howMany"

						Calculator.this.doCalc();
					}
				}

				public static class CalcInnerStat {

					public void doNothing() {

					}
				}

				public void doCalc() {
					System.out.println("Cacled!");

					new BackgroundWorkApplier().apply(() -> System.out.println(Calculator.this.howMany));

					CalcInner calcInner = new CalcInner();
					CalcInnerStat calcInnerStat = new CalcInnerStat();
				}
			}

			class App {

				public static void main(String[] args) {
					Calculator.CalcInner calcInner = new Calculator().new CalcInner();
					calcInner.doSomething();

					Calculator.CalcInnerStat calcInnerStat = new Calculator.CalcInnerStat();
					calcInnerStat.doNothing();
				}
			}


			@FunctionalInterface
			interface BackgroundWorker {
				void doWork();
			}

			class BackgroundWorkApplier {

				public void apply(BackgroundWorker worker) {
					worker.doWork();
				}
			}
		

		C#:

			class Program
			{
				static void Main(string[] args)
				{
					Calculator calculator = new Calculator();
					Calculator.CalcInner calcInner = new Calculator.CalcInner(calculator);

					// can have only static methods
					Calculator.CalcInnerStat.DoNothing();
				}
			}

			public class Calculator
			{
				private int _howMany = 10;

				public class CalcInner
				{
					private readonly Calculator _calculator;
					public CalcInner(Calculator calculator)
					{
						_calculator = calculator;
					}

					public void DoSomething()
					{
						// see's the private fields, but the Calculator instance has to be provided in the ctor
						Console.WriteLine(_calculator._howMany);
					}
				}

				public static class CalcInnerStat
				{
					public static void DoNothing()
					{

					}
				}

				public void DoCalc()
				{
					ExecutionContext.Run(ExecutionContext.Capture(), _ =>
					{
						var _nothing = new Calculator();
						// this points to the Calculator instance here
						Console.WriteLine(this._howMany);

						// "this" has to be passed
						var calc = new CalcInner(this);
					}, null);
				}
			}
	


********JMX*********

JMX: Java Management Extensions
JConsole: it is a JMX implementation, baked into the java bundle (JVM_HOME/bin folder --> jconsole.exe).
	JConsole uses the extensive instrumentation of the Java Virtual Machine (Java VM) to provide information about the performance and resource consumption of applications running on the Java platform.

	
********Garbage Collection*********

The Heap is divided into young and old generations as follows:
	Young Generation: It is place where lived for short period and divided into two spaces:
		Eden Space (there is 1 of this): When object created using new keyword memory allocated on this space. If the object survives the 1st garbage collection, it goes to the Survivor space.
		Survivor Space (there are 2 of this): This is the pool which contains objects which have survived after java garbage collection from Eden space. Objects may stay here for several garbage collection cycle.
			If the survivor space gets full, it is copied to the other survivor space. Of that one gets full, the longest lived objects goes to the Old Generation.
		
	Old Generation: This pool basically contain tenured and virtual (reserved) space and will be holding those objects which survived after garbage collection from Young Generation.
		Tenured Space: This memory pool contains objects which survived after multiple garbage collection means object which survived after garbage collection from Survivor space.
		
	Permanent Generation: This memory pool as name also says contain permanent class metadata and descriptors information so PermGen space is always reserved for classes and those that is tied to the classes for example static members.

	Java8 Update: PermGen is replaced with Metaspace which is very similar.
		Main difference is that Metaspace re-sizes dynamically i.e., It can expand at runtime.
		Metaspace is unmanaged C code while PermGen is managed Java.
		Java Metaspace space: unbounded (default)
		
	Code Cache (Virtual or reserved): If you are using HotSpot Java VM this includes code cache area that containing memory which will be used for compilation and storage of native code
	
	
	
********JNDI*********

It's a way of looking up keys and values in a context which is common in a lot of application servers
In case of JNDI Spring does not instantiate the object, instead it manages an object in the context by looking up the object in some other memory of the JVM.
In case of an application server, looks up the service from the memory of the application server.



**********Native DLL at runtime***************

https://www.chilkatsoft.com/p/p_499.asp
https://examples.javacodegeeks.com/java-basics/java-library-path-what-is-it-and-how-to-use/
	
	java -Djava.library.path=<path_to_dll> <main_class>				- works
	System.setProperty(“java.library.path”, “/path/to/library”);	- does not work because the "java.library.path" is readonly but this is not well documented. Can be only set at the JVM startup.
	
	Java System.loadLibrary - Loading a Native DLL at Runtime
		System.loadLibrary("jri"); - will search for "chilkat.dll" in the folders specified by the PATH env variable
		System.load("C:/Program Files/R/R-3.3.3/library/rJava/jri/x64/jri.dll"); - this can be used if we do not want to bother setting the PATH env variable
		
		
		
	System.getProperty("java.class.path") - to get the classpath specified and used
	


**********Concurrency, multithreading***************

Prefer to use ForkJoinPool over Executors. 
	- ForkJoinPool implements the Executor interface: both are Executors
	- ForkJoinPool was added in Java7 whereas Executors in Java5
	- ForkJoinPool by default has as many threads in its pool as the physical number of CPU cores (typically default)
		- To change this: new ForkJoinPool(100)
	
	https://stackoverflow.com/questions/33944471/forkjointask-vs-completablefuture
	
CompletableFuture:
	- this is a Promise
	- added in Java8
	- it uses the ForkJoinPool.commonPool() which has the same number of threads as the number of CPU cores
	- this is the preferred way to initiate asynchronous tasks
	- CompletableFuture.supplyAsync(()-> supplier, executor).thenAccept(...);
	- CompletableFuture.supplyAsync(()-> supplier).thenAccept(...); // default ForkJoinPool is used
	
	

**********32-bit vs 64-bit compilation**********

Java bytecode is java bytecode, it doesn't matter whether it was built with a 32-bit or 64-bit JDK and there is no way to figure this out.

I think it does not make any difference to have a jar compiled with 32-bit or 64-bit. It should be machine-independent - the JVM will compile it to native 32/64-bit code;
Unless you have some native library dependency or the java code is directly being compiled to native code.

A pure jar is not compiled to a particular architecture.
A 32-bit JVM will run the jar in 32-bit, likewise a 64-bit JVM will run the jar in 64-bit.
Of course, if your jar uses native libraries then the 'bitness' of these will have to match the JVM that you use to run the jar.


**********Java 9***************

Running a program:
	- JDK8:
		javac xxx.java
		java xxx.class
		java -jar xxx.jar
	- JDK9:
		java -p mods -m hello.world
			-p: points to the directory of modules (module path!)
			-m: name of the module to execute
		classpath does not need to be set!
			- set the module path instead (the same as the classpath)
		no missing dependencies
		no cyclic dependencies
		no split packages
			- a package can belong to exactly 1 module
			- this is the mess we get with the classpath:
				- packages can reside in multiple jar files
				- performance bottleneck: all jar files on the classpath have to be scanned for the required package when loading a class
					- we load some classes from one jar, and other classes from the other jar!
		modules are not mandatory - classpath still exists!


Jigsaw's goals:
	- Provide stronger encapsulation of components that extend beyond what the JDK currently provides.
	- Reliable configuration of modules in an application.
	- Reduce the footprint of the run-time image for an application to what is only needed.
	
A module is a jar file which declares it’s dependencies and “public” API via a module descriptor file named module-info.java. The module-info.java file specifies the following:
	- The name of the module.
	- The packages it exports.
	- Other modules it depends on.
A module is a set of packages designed for reuse.
	
Module Descriptor: (module-info.java)
	- The module descriptor is the module-info.java file. It specifies the public API of the module. (ie: what it requires and what it depends on)
	- Required modules are NOT transitively available to transitive consumers, (ie: A requires B requires C means A does not see C automatically) unless requires transitive is specified. 
	- Modules are required and packages are exported.

Tools:
	- JDeps: 
		- A Java command line driven static dependency management tool for jar files. Results can be filtered and packaged at package level or jar file level.
		- It can be found in the bin folder of your JDK and has been around since Java 8. Confirming the version of jdeps is as easy as running
		- jdeps --version.
		- makes it possible to analyse our project's static dependencies
		- can generate module descriptors (module-info.java) for non-module jars
		- runs on bytecode, not on sourcecode
	- JLink:
		- A Java command line driven tool which links / brings together all required modules for an application into a run time image.
		- This image is usually drastically smaller in size, thus helping reduce the footprint of the application as the entire JRE is not normally needed. jlink will also resolve static dependencies between modules thus guaranteeing the integrity of each module at run time. jlink requires that all artifacts are modules with well defined public contracts (exports, requires etc), thus “Automatic” modules will not suffice.
		- jlink --version
	-JShell:
		- This command line environment brings a form of Read-Eval-Print-Loop (REPL) to the Java platform. This is intended to facilitate prototyping and exploration of coding options with immediate results and feedback.
		- Somewhat like the C# Interactive window in Visual Studio 2017, but without the IDE!
	
Module Types:
	- Platform modules
		- the JDK is diveded into platform modules
		- exports packages: explicitly
	- Application Modules
		- any JAR containing module-info.class on the module path
		- Also referred to as “Named Application Modules"
		- This is what we are going to create. A lot of our third party dependencies will also be application modules
		- exports packages: explicitly
		- can read modules: Platform, Application, Automatic
	- Automatic Modules
		- any JAR without module-info.class on the module path, allowing your Java 9 project to use pre-Java-9 libraries
		- for the name, JDK generates one depending on the JAR filename. Basically, it will remove the file extension and the trailing version number (if any), and replace all non-alphanumeric characters by dots, e.g. the file mongo-java-driver-3.3.0.jar will end up as module named mongo.java.driver
		- export packages: all
		- can read modules: Platform, Application, Automatic, Unnamed
	- Unnamed Module
		- all JARs and classes on the classpath. The classpath is not completely gone yet.
		- Similar to automatic modules, it exports all packages and reads all other modules
		- But it does not have a name, obviously
		- export packages: all
		- can read modules: Platform, Application, Automatic

Unnamed module:
	- If a request is made to load a type whose package is not defined in any known module then the module system will attempt to load it from the class path
	- If this succeeds then the type is considered to be a member of a special module known as the unnamed module
		- to ensure that every type is associated with some module
	- The unnamed module reads every other module
		- will thus be able to access the exported types of all other readable modules
	- The unnamed module exports all of its packages
	
Services:
	- Java has long supported services via the java.util.ServiceLoader class, which locates service providers at run time by searching the class path
	
	module java.sql {
		requires public java.logging;
		requires public java.xml;
		exports java.sql;
		exports javax.sql;
		exports javax.transaction.xa;
		uses java.sql.Driver;										- this module uses the Driver (this is an interface)
	}
	
	module com.mysql.jdbc {
		requires java.sql;
		requires org.slf4j;
		exports com.mysql.jdbc;
		provides java.sql.Driver with com.mysql.jdbc.Driver;		- this module provides the implementation of the Driver (class and interface)
	}
	
	
	requires: module name goes here
	exports:  package name goes here
	provides: concrete class/interface goes here
	with:	  concrete implementation class goes here
	uses: 	  concrete class/interface goes here
	
	ServiceLoader.load(EventService.class): 
		- the call to ServiceLoader.load(Class<T>) returns an Iterable<T>, containing all service implementations of a given interface 
		  that are being offered by modules on the modulepath using the provides ... with ... statement.
		  
Class loaders (for modules):
	- Every type is in a module, and at run time every module has a class loader - but does a class loader load just one module?
		- A class loader can load types from one module or from many modules, so long as the modules do not interfere with each other
		- and all of the types in any particular module are loaded by just one loader.
	- it allows us to retain the platform’s existing hierarchy of built-in class loaders
		- The bootstrap and extension class loaders still exist, and are used to load types from platform modules
		- The application class loader also still exists, and is used to load types from artifacts found on the module path
	
	
New language features:

	1. try-with-resources:
	
		final Resource r = new Resource();
		try (r) {
			...
		}
		
	2. underscore is no longer an identifier name --> syntactic name reclamation
	
		int _ = 4;	// compile error
		new Foo<String, _>();
		foo.<String, _>bar();
		
	3. private interface methods
	
		- can be static methods, instance methods as well
		- usage: helper methods to implement default methods
		
	4. Lambda Stream and Optional API extended
	
	5. Collections framework enhancements
		
		List.of("a", "b", "c"); 			instead of: Collections.unmodifiableList(Arrays.asList("a", "b", "c"));	--> 
		Set.of() 							instead of: Collections.unmodifiableSet
		Map.of()							instead of: Collections.unmodifiableMap(map)
		
		Map.entry, Map.ofEntries
		
		These static factory methods consume much less disk space than the old way and have less object allocations to initialize an unmodifiable collection.
		
	
Migrating to JDK9:
	- Java EE modules: there are some modules that both ship as part of Java SE and as part of an application server / servlet container (Tomcat for example)
	- these modules (corba, jaxb, jaxws, common annotation) are deprecated in JDK9 and will be removed in a future release
		- disabled by default in JDK9 (use --add-modules if they are needed)
		
Garbage collection:
	- G1 became the default garbage collector in JDK 9 (replacing the Serial and Parallel collectors)
		- Serial and Parallel collectors: the heap is divided into 2 or 3 regions (young, old generation, metaspace (or the legacy permanent generation))
		- with G1:
			- the heap is divided into more regions, usually around 2.000 or so
			- unlike Serial and Parallel, the marking phase (finding the live objects) is done concurrently, without stopping the Java application
			- can therefore collect a few regions at a time
				- fewer objects to collect --> shorter pauses
				- more information --> more control over pauses
	- for most scenarios it will deliver better performance