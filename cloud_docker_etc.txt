service provider

Cloud:

	https://www.youtube.com/watch?v=qVYMQwSbTSk - the definition for private cloud types

	- Public cloud:
		- service is accessible to multiple customers over the public internet
		- mutitenancy:  makes some people nervous because mutliple customers sharing the exact same resources
		
	- Virtual private cloud:
		- set of resources on a public cloud
		- are walled off virtually and assigned to you
		- we're still on a multitenant environment, but we have some security constraints around what we do
		- fairly easy
	
	- Hosted private cloud
		- actually walled off
		- don't put you on a public internet
		- create an environment specific to you
		- operates the cloud for you, you use it as it was your own personal resources
	
	- Private cloud:
		- you put the cloud infrastructure and automation on top of resources that are in your own datacenter
		
	The implication between all of these are both security related and cost related.
		- Private cloud: 2 major implications
			- your IT department must run it: you must have the expertise and understanding how to run a cloud - there are very few enterprises that really know how to do it
			- you have to provide the cloud environment 
	
	
Basically 2 ways to build applications nowadays:
	- monolith apps
		- advantages:
			- It is well known
			- Have faster calls between components since they are often on the same machine
			- Easy deployment: typically, only a single archive needs to be copied
		- disadvantages:
			- Harder code maintenance: the application will eventually start to break down as the team grows, experienced developers leave and new ones join in.
			- The application is tightly coupled within the tiered layers, and you cannot scale individual components.
			- Limited agility: because of the tighter coupling, a little change in a feature may easily introduce new bugs in the dependent features. The whole application needs to be tested again which can be time-consuming.
			- Stuck with technology stack: it is typically not possible to change the technology stack mid-stream without throwing away or rewriting significant part of the existing application.
			- Technical debt: a monolith application is built over several years with the team that is maintaining the code base completely different from the one that created the application. This increases technical debt of the application and makes it that much harder to refactor the application later on

	- apps with microservices architecture
		-advantages:
			- Easier to develop, understand, and maintain: code in a microservice is restricted to one function of the business and is thus easier to understand.
			- Starts faster than a monolith: scope of each microservice is much smaller than a monolith and this leads to a smaller archive
			- Scale independently: each service can scale independently using X-axis cloning and Z-axis partitioning based upon their need. This is very different from monolithic applications that may have very different requirements and must be deployed together.
			- Improves fault isolation: a misbehaving service, such as with a memory leak or unclosed database connections, will only affect that service as opposed to the entire monolithic application. This improves fault isolation and does not bring the entire application down, just a piece of it.
			- No long term commitment to any stack: developers are free to pick language and stack that is best suited for their service. Even though the organizations may restrict the choice of technology but you are not penalized because of past decisions. It also enables to rewrite the service using better languages and technologies. This gives freedom of choice to pick a technology, tools, and frameworks.
			- Light-weight communication: services communicate with each other using a light-weight communication, such as REST over HTTP. This is inherently synchronous and so could have some potential bottlenecks. An alternative mechanism is to use publish-subscribe mechanism that supports asynchronous messaging
		- However, with microservices, there comes additional requirements that the infrastructure must fulfil:
			- Service replication: each service needs to replicate using X-axis cloning or Z-axis partitioning.
			- Service discovery: services need to be discovered somehow. Multiple services might be collaborating to provide an application functionality; therefore, they need to know about each other.
			- Resiliency: failure in software occurs, no matter how much and how hard you test it. The key question is not “how to avoid failure” but “how to deal with it”. This is all the more prominent in microservices where services are distributed all over the Internet. It is important for services to automatically take corrective action and ensure the user experience is not impacted.
			- Service monitoring: one of the most important aspects of a distributed system is service monitoring and logging. This allows to take a proactive action, for example, if a service is consuming unexpected resources


		
	
Micro-services:
	Spring Cloud provides tools for developers to quickly build some of the common patterns in distributed systems (e.g. configuration management, service discovery, circuit breakers, intelligent routing, micro-proxy, control bus, one-time tokens, global locks, leadership election, distributed sessions, cluster state).
	Coordination of distributed systems leads to boiler plate patterns, and using Spring Cloud developers can quickly stand up services and applications that implement those patterns. They will work well in any distributed environment, including the developer’s own laptop, bare metal data centres, and managed platforms such as Cloud Foundry.
	
	Features: Spring Cloud focuses on providing good out of box experience for typical use cases and extensibility mechanism to cover others.
	- Distributed/versioned configuration
	- Service registration and discovery
	- Routing
	- Service-to-service calls
	- Load balancing
	- Circuit Breakers
	- Global locks
	- Leadership election and cluster state
	- Distributed messaging
	
Eureka, ZooKeeper: service registry and provide network discovery (prefer to use eureka since it is more resilient to failures)
Hystrix: Fault Tolerance: provide fallback methods in case of failures - on the edge service, microservices. Implements the circuit breaker pattern.
Microservice: invoked by the edge service, and they can invoke each other (hystrix needed if invoking other services for the best user experience - avoid showing stacktraces)
Zuul: egde service implementation: micro proxy, api gateway
	1. Micro proxy: blindly forward packages outside the datacenter inside the loadbalancer
	2. API gateway: transforms the request to the services behind the loadbalancer
Actuator: monitoring capabilities via jmx and rest api (use with the config server)
Config server: central store for configurations of the participants of the system
Hystrix dashboard, Turbine: use for health monitoring
	Turbine: aggregates the Hystrix health monitoring data from each method of each microservice and provides an aggregated result.
Ribbon: round robin load balancer
Zipkin: Zipkin is a distributed tracing system. It helps gathering timing data needed to troubleshoot latency problems in microservice architectures.
Kafka: Kafka™ is used for building real-time data pipelines and streaming apps. It is horizontally scalable, fault-tolerant, wicked fast, and runs in production in thousands of companies.

Config service: just a super lightweight service that is used to centralize the configuration data 
	Scenario is that I have multiple microservices and do not want to store config one by one for each service, rather I would store them in a 
	central config server.
	
Messaging: point-to-point service communication is good for reading, however for writing which requires synchronization is not good.
		   We could use distributed transations but Josh Long urges us not to, it just slows down the aggregate system.
		   Instead use messaging: forward to the service on a message bus: job is put into a queue (buffer) and eventually gets picked up by the service.
	- JMS: only between Java-based components
	- AMQP (Advanved Message Queuing Protocol): works between components implemented with different technologies (.NET, Java, Python, ...)
	
Hystrix dashboard, H2 console - good stuff, start.spring.io starters
Spring cloud handles Single Sign on, Oath2 is dead simple


********************************
Microservices transactions vs eventual consistency
********************************

2PC
	+ ensures consistency
	- in exchange for the system is less available
	- not supported by NoSQL databases
	Avoid this in microservices as today availability is more preferred than consistency.
	
Messaging
	+ availability
	- the system is not consistent right now but eventually it will be
	This is the way to go with microservices when an update needs to be done that spans multiple microservices.
	
Reliably publish events when state changes:
	1. Application events: 		the very same transaction that updated the database also places a new record into a dedicated "Events" table. So this is still ACID.
								Another thread continuously pulls for new records and sends corresponding messages to a message broker.
								
	2. Transaction log tailing: interesting
	
	3. Event sourcing: 			event-centric persistence: only events are used. The database and message broker are merged into an Event Store.
								Storing a sequence of events instead of the current state.
								Storing events is an append-only operation.
								Event Sstore stores the events and publishes them as well to consumers (other microservices for example).
								Event Store = database + message - database could be SQL or NoSQL.
								
								1. Specify domain events to each domain object. For example: Order domain object's events are:
																										- OrderCreated
																										- OrderCancelled
																										- OrderApproved
																										- OrderRejected
																										- OrderShipped
								2. Persist events NOT current state!
									- Instead of placing Order entity into Order table, place the event!
									- we have Event table: 	|EntityId|Entity type|Event id|Event type|Event data
															|101     |Order      |OrderCreated       | ...
															|101     |Order      |OrderApproved      | ...
									- so we just store the sequence of events
									- the Event Store also publishes the events, so that consumers (other microservices) can act as necessary.
									
								3. When I want to load the Order entity:
									- query the events from the Event table for a given Order
									- replay those events - OrderCreated(), OrderApproved()
									- if there are lots of events use snapshotting
									
									currentState = foldl(applyEvent, initialState, events)
									
								Drawbacks:
									- Querying the Event Store can be challenging
									- Some queries might be complex/inefficient, e.q. accounts with a balance > X-axis
									- Event Store might only support lookup of events by entity id
									- Must use Command Query Responsibility Segregation (CQRS) to handle queries --> 
										- application must handle eventualy consistent data
										- denormalized views to the rescue - for example storing the view of the Order events in an Order document in MongoDb database
											- MongoDb is kept up to date by subscribing to events published by the Event Store
											
Summary
	- Aggregates are the building blocks of microservices
	- Use events to maintain consistency between aggregates
	- Event sourcing is a good way to implement an event-driven architecture



********************************
Spring cloud deployment
********************************

Cloud Foundry: PaaS platform
BOSH: how you automate, install and congfigure a CF system (among other things, it does a lot of stuff)

	
	
Containers vs Virtual Machines:
	Application Storage Overhead
		- Virtual Machines: Each application workload runs on a separate virtual machine – which typically needs Gigabytes of OS overhead.
		- Containers: Containers can run in isolation on the same physical or virtual machine – in which one common OS is used for all containers. The Docker Engine overhead is pretty small – i.e. megabytes.
		
	Instantiation
		- Virtual Machines: Boot up time of both the OS and the application
		- Containers: Application instantiation time only – i.e. time to spin up Linux processes
	
	Resource Allocation
		- Virtual Machines: Generally rigid. Virtual CPU and Memory are typically pre-allocated to a VM and are hard to change post-provision – depending on the virtualization platform or cloud provider used.
		- Containers: Generally flexible. CPU and Memory maximum limits can be defined for containers if desired (using the cpu_shares and mem_limit parameters). By default, containers can continue to consume the resources available on the underlying machine. With HyperForm, the minimum limits for CPU and Memory can also be defined — allowing users to place application workloads on machines that have enough CPU and Memory resources. You can read more about this here: http://dchq.co/docker-compose.html#docker-nodejs.
	
Cloud Foundry vs Docker:
	- Docker is a technology for creating and running Linux "containers." A docker container for SpringBoot app will consist of a docker image, which will basically contain a filesystem with all the things needed to run your app (JVM, your source code, etc.), and docker container metadata, which tells the docker daemon how to run the app inside the image (e.g. what environment variables to set, what ports to expose, what commands to run, etc.). The docker daemon will use Linux features such as groups and kernel namespaces to run the container in isolation from other processes running on the host machine. Docker is somewhat low-level, in that you need to specify everything that goes into the image, and it runs arbitrary things, namely whatever you put into your image and tell it to run. The docker container that you get is very portable, so you can build, test, and run your docker container locally for development, and then ship that container to a production host that also has a docker daemon running on it, and be quite confident that you're getting the exact same thing.
	
	- Cloud Foundry works at a higher layer of abstraction, with applications being a first class concept. Cloud Foundry uses containerization technology similar to docker to build portable images and then run them, but it's an implementation detail and you don't need to specify all the details. In newer versions of Cloud Foundry, docker images will also be supported so you can specify the details if you want, but it also has a "buildpack" workflow, where it will automatically detect a Java application when you push your app and will know to include all the things necessary for the Java runtime when it builds the image.
	
	With Cloud Foundry, since applications and application management are first class concepts, and since it operates at a higher level, you get all sorts of things for free. For instance, you can easily scale your app horizontally (add instances), e.g. cf scale my_app -i 5 or vertically, cf scale my_app -m 2G (to set the allocated memory for each instance). You get streaming application logs: cf logs my_app. Cloud Foundry gives you a lot of fault tolerance for free, so if one of your application instances crashes, or the process running the application containers itself crashes (the thing that's similar to the docker daemon), or if the host VM that's running the container-running process dies, or the hardware cluster where that VM resides dies, Cloud Foundry will automatically bring your instances back up.

	The docker daemon is a single process you can run on any Linux machine. So if you're doing something small and simple, and you need to do most of the setup yourself, it can be easier to get up and running both locally and in development using docker. With docker it's also easier to have access and share the docker image you create, so once you've created an image, you can put it in a docker repository, and then you can run it on any other docker daemon. With Cloud Foundry, the built image is generally an implementation detail and you don't really have access to it, so for instance you couldn't extract that image and run it on another Cloud Foundry installation.

	There are various projects out there intended to make Cloud Foundry more accessible and easier to set up, while still giving you many of the benefits of a PaaS. Some of these projects also aim to allow you to combine using docker and the benefits of docker while also getting a lot of the PaaS benefits you get from Cloud Foundry.
	
	
	
Docker:

	Dockerfile --> (docker build) --> image --> (docker run) --> container
																(docker stop, docker start)
					(docker pull) --> download ready-to-use image from Docker Hub

	Daily cmd commands:
		docker --help
		docker version
		docker info - prints info about the host VM: OS, OS version, RAM, CPU, containers, running containers, docker root dir, ...
	
		docker pull [image_name]] - to pull existing image from Docker Hub
		docker build -t my_mongodb . - to create an image named "my_mongodb" from the Dockerfile script located in the same folder
			-t [name of image] - name of the iamge
			. location of the Dockerfile?
		docker run [image_name] - to create a new contaimer from the image (copies the image into a folder to the disk) 
			- the last string segment must be the name of the image!
			- this command can be run multiple times resulting in more instances (containers) created
			- but with this we would have to deal  with compley IDs instead of friendly names
			
			- docker run --name my-mongo_1 -d my_mongodb
				--name [container_instance_name]
				-d - detach, do not attach to the console of the container
			
			docker run -d -p 127.0.0.1:81:80 --name webserver nginx - the same as:
			docker run -d -p 81:80 --name webserver nginx
				-p - this binds port 80 of the container to port 81 on 127.0.0.1 of the host machine
				The p swtich CANNOT be used in Dockerfile scripts because that would be a security issue and only local administrators can set which port gets publish on the container to which port of the host OS. To avoid long "docker run" commands, consider using docker-compose: http://stackoverflow.com/questions/32740344/how-to-publish-ports-in-docker-files
					mywebserver:
					  image: nginx
					  ports:
						- 81:80
						
				since containers are isolated I can easily spin up another instace of the "nginx" image but this time binding to different port on the host OS to avoid port collision:
					docker run -d -p 82:80 --name webserver2 nginx
				
			docker run --expose 80 --name webserver nginx
				--expopse - this exposes port 80 of the container without publishing the port to the host system’s interfaces.
		docker start [container_id] - starts the container
	
		docker stop [container_id] - to stop the container
		docker rm [container_id] - to remove the container
		docker rmi [image_id] - to remove the image
		
		docker commit c3f279d17e0a  svendowideit/testimage:version3 
			- create a new image which name is "vendowideit/testimage", tag is "version3" using the container which id is "c3f279d17e0a"
		docker cp - copy files/folders from the containers filesystem to the host path
		
		docker search [string] - to search for images
		docker pull [image_name] - to download the image from Docker Hub
		docker push - to push image back to Docker Hub and share it with other folds
		
		docker images - list all images available
		docker ps - list running containers
		docker ps -- all - list all containers
		
	Container, images location:
		- if using linux containers, the location is the MobyLinuxVM.vhdx that is automatically started using Hyper-V
			/var/lib/docker
		- if using windows containers, C:\ProgramData\Docker\ folder is the destination 
			there are "image", "containers", "swarm" folders though these ones contain only metadata (folder name is the identifier)
			"windowsfilter" folder contains the real container filesystem (pick one based on the identifier --> "Files" and there you see the installed filesystem (windows OS is shared with the host and linux is not supported), the libraries (php, inetpub iis) and your application
			
	Docker related commands (to be used in Dockerfile scripts)
		ADD
			The ADD command gets two arguments: a source and a destination. It basically copies the files from the source on the host into the container's own filesystem at the set destination. If, however, the source is a URL (e.g. http://github.com/user/file/), then the contents of the URL are downloaded and placed at the destination.
			# Usage: ADD [source directory or URL] [destination directory]
			ADD /my_app_folder /my_app_folder
			
		CMD
			The command CMD, similarly to RUN, can be used for executing a specific command. However, unlike RUN it is not executed during build, but when a container is instantiated using the image being built. Therefore, it should be considered as an initial, default command that gets executed (i.e. run) with the creation of containers based on the image.

			To clarify: an example for CMD would be running an application upon creation of a container which is already installed using RUN (e.g. RUN apt-get install …) inside the image. This default application execution command that is set with CMD becomes the default and replaces any command which is passed during the creation.
			# Usage 1: CMD application "argument", "argument", ..
			CMD "echo" "Hello docker!"
			
		ENTRYPOINT
			ENTRYPOINT argument sets the concrete default application that is used every time a container is created using the image. For example, if you have installed a specific application inside an image and you will use this image to only run that application, you can state it with ENTRYPOINT and whenever a container is created from that image, your application will be the target.

			If you couple ENTRYPOINT with CMD, you can remove "application" from CMD and just leave "arguments" which will be passed to the ENTRYPOINT.
			# Usage: ENTRYPOINT application "argument", "argument", ..
			# Remember: arguments are optional. They can be provided by CMD
			#           or during the creation of a container. 
			ENTRYPOINT echo

			# Usage example with CMD:
			# Arguments set with CMD can be overridden during *run*
			CMD "Hello docker!"
			ENTRYPOINT echo  
		
		ENV
			The ENV command is used to set the environment variables (one or more). These variables consist of “key = value” pairs which can be accessed within the container by scripts and applications alike. This functionality of docker offers an enormous amount of flexibility for running programs.
			# Usage: ENV key value
			ENV SERVER_WORKS 4
			
		EXPOSE
			The EXPOSE command is used to associate a specified port to enable networking between the running process inside the container and the outside world (i.e. the host).
			# Usage: EXPOSE [port]
			EXPOSE 8080
			
		FROM
			FROM directive is probably the most crucial amongst all others for Dockerfiles. It defines the base image to use to start the build process. It can be any image, including the ones you have created previously. If a FROM image is not found on the host, docker will try to find it (and download) from the docker image index. It needs to be the first command declared inside a Dockerfile.
			# Usage: FROM [image name]
			FROM ubuntu
			
		MAINTAINER
			One of the commands that can be set anywhere in the file - although it would be better if it was declared on top - is MAINTAINER. This non-executing command declares the author, hence setting the author field of the images. It should come nonetheless after FROM.
			# Usage: MAINTAINER [name]
			MAINTAINER authors_name
			
		RUN
			The RUN command is the central executing directive for Dockerfiles. It takes a command as its argument and runs it to form the image. Unlike CMD, it actually is used to build the image (forming another layer on top of the previous one which is committed).
			# Usage: RUN [command]
			RUN aptitude install -y riak

		USER
			The USER directive is used to set the UID (or username) which is to run the container based on the image being built.
			# Usage: USER [UID]
			USER 751

		VOLUME
			The VOLUME command is used to enable access from your container to a directory on the host machine (i.e. mounting it).
			# Usage: VOLUME ["/dir_1", "/dir_2" ..]
			VOLUME ["/my_files"]
			
			docker run -d --name web -v /src/webapp:/webapp	# mount the host's /src/webapp folder to the container's /webapp folder. When the files change on the host they are automatically refresh in the container as well without the need to build a new image and start it. Later we can "docker commit" the modified container to a new/existing image

		WORKDIR
			The WORKDIR directive is used to set where the command defined with CMD is to be executed.
			# Usage: WORKDIR /path
			WORKDIR ~/
		
		
	Docker project (open-sourced by dotCloud in March '13) consists of several main parts (applications) and elements (used by these parts) which are all [mostly] built on top of already existing functionality, libraries and frameworks offered by the Linux kernel and third-parties (e.g. LXC, device-mapper, aufs etc.).

	Main Docker Parts
		1. docker daemon: used to manage docker (LXC) containers on the host it runs (this is the docker engine?)
		2. docker CLI: used to command and communicate with the docker daemon
		3. docker image index: a repository (public or private) for docker images
		
	Main Docker Elements
		1. docker containers: directories containing everything-your-application
		2. docker images: snapshots of containers or base OS (e.g. Ubuntu) images
		3. Dockerfiles: scripts automating the building process of images
		
	Each container is layered like an onion and each action taken within a container consists of putting another block (which actually translates to a simple change within the file system) on top of the previous one. And various tools and configurations make this set-up work in a harmonious way altogether (e.g. union file-system).

	Docker containers:
		Docker containers are basically directories which can be packed (e.g. tar-archived) like any other, then shared and run across various different machines and platforms (hosts). The only dependency is having the hosts tuned to run the containers (i.e. have docker installed). Containment here is obtained via Linux Containers (LXC).
		
		Docker containers have several main features.
			They allow:
				Application portability
				Isolating processes
				Prevention from tempering with the outside
				Managing resource consumption
				
			They do not allow:
				Messing with other processes
				Causing "dependency hell"
				Or not working on a different system
				Being vulnerable to attacks and abuse all system's resources
				
	Dockerfiles
		Each Dockerfile is a script, composed of various commands (instructions) and arguments listed successively to automatically perform actions on a base image in order to create (or form) a new one.
		
	.dockerignore file
		Before the docker CLI sends the context to the docker daemon, it looks for a file named .dockerignore in the root directory of the context. If this file exists, the CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using ADD or COPY.
				
	Being based and depending on LXC (Linux containers), from a technical aspect, these containers are like a directory (but a shaped and formatted one). This allows portability and gradual builds of containers.
	
	Getting started on Linux:
		1. instal docker
		2. run the docker daemon: sudo docker -d &
		3. Using docker (via CLI) consists of passing it a chain of options and commands followed by arguments. Please note that docker needs sudo privileges in order to work.: sudo docker [option] [command] [arguments]

		
	Searching for images in the docker repository:
		docker search ubuntu
		docker search windows
	
	
	Committing changes to an image: docker commit [container ID] [image name]
		As you work with a container and continue to perform actions on it (e.g. download and install software, configure files etc.), to have it keep its state, you need to “commit”. Committing makes sure that everything continues from where they left next time you use one (i.e. an image).
		
		Then this image can be pushed into Docker Hub: docker push [username/image name] 
		Later on Dockerfile scripts can perform "FROM" this image after being pulled down (docker pull [image_name])
		
Docker Compose

	Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file (docker-compose.yml) to configure your application’s services. Then, using a single command, you create and start all the services from your configuration.
	
	Using Compose is basically a three-step process.
		1. Define your app’s environment with a Dockerfile so it can be reproduced anywhere.
		2. Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.
		3. Lastly, run docker-compose up and Compose will start and run your entire app.
		
	Daily commands:
		docker-compose --help
		docker-compose version
		docker-compose up - to start the images either from Docker Hub or build them from Dockerfile then start
			docker-compose up -d <-- to detach the terminal and run the services in the background
		docker-compose ps - to see what is curerntly running	
		docker-compose down - to stop then even remove the containers (brings down everything)
			docker-compose down --volumes - to remove the volumes as well
		docker-compose run - allows you to run one-off commands for your services
			For example, to see what environment variables are available to the "web" service: docker-compose run web env
			
	docker-compose related commands (to be used in docker-compose.yml files)
		build: . - use the current folder and look for "Dockerfile"
		
		build:
		  context: ./dir 						- context is a path to a directory containing a Dockerfile
		  dockerfile: Dockerfile-alternate 		- name of the Dockerfile which is not the default (ofc the default "Dockerfile" can be specified as well but it is pointless
		  
		build: ./dir
		image: webapp:tag - this will result in an image named webapp and tagged tag, built from ./dir
			without specifying the image and building from a Dockerfile (instead of using an image from the Docker Hub) the built image's name is going to be {folder_name_of_docker-compose_file}_{service_name}
		
		container_name - Specify a custom container name, rather than a generated default name.
			without specifying the container_name the default name of the contaimer is going to be {image_name}_1
		
		expose - expose ports without publishing them to the host machine - they’ll only be accessible to linked services. Only the internal port can be specified.
		image - Specify the image to start the container from. Can either be a repository/tag or a partial image ID.
				In this case the "build" command is not specified.
		ports - publishes ports on the container to the host OS so it can be reached from the host
				Either specify both ports (HOST:CONTAINER), or just the container port (a random host port will be chosen).
		volumes - Mount host paths or named volumes. Named volumes must be defined in the top-level volumes key.
				  If we make modifications into the mounted on the host OS, then we see changes immediately without having to rebuild the image.
		
		
	Example:
		version: '2'
		services:
		  web:
			build: .
			ports:
			 - "5000:5000"
			volumes:
			 - .:/code			# mount the host's current directory to the container's /code directory. Whenever we change something in the current dir that is added to the container (ADD) the changes are automatically populated without the need to do "docker-compose down" then "docker-compose up"
		  redis:
			image: "redis:alpine"
			
		This Compose file defines two services, web and redis. The web service:
		- Uses an image that’s built from the Dockerfile in the current directory.
		- Forwards the exposed port 5000 on the container to port 5000 on the host machine.
		- Mounts the project directory on the host to /code inside the container, allowing you to modify the code without having to rebuild the image.
		The redis service uses a public Redis image pulled from the Docker Hub registry.
		
		Because the application code is mounted into the container using a volume, you can make changes to its code and see the changes instantly, without having to rebuild the image.
			1. Change the greeting in app.py and save it. For example: return 'Hello from Docker! I have been seen {} times.\n'.format(count)
			2. Refresh the app in your browser. The greeting should be updated, and the counter should still be incrementing.
			
		As with docker run, options specified in the Dockerfile (e.g., CMD, EXPOSE, VOLUME, ENV) are respected by default - you don’t need to specify them again in docker-compose.yml.